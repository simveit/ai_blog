{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0314a31b-f5d0-438a-84dd-cde6ce3be58f",
   "metadata": {},
   "source": [
    "# Improving our ü§ó base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5bde4-0fc1-4469-997a-0eeddd9cb033",
   "metadata": {},
   "source": [
    "After training our first model with ü§ó we want to explore ways to improve our training and make the notebook better readable.\n",
    "\n",
    "- In a first step we use DistilBERT Base model as it's small, fast and good for our üåç."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5810be35-c7aa-4c56-946f-786de23f39aa",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124c102-1c88-419f-b4e6-328c6d160bfc",
   "metadata": {},
   "source": [
    "As always we first import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d26e666-591a-4ec3-9b2c-5ef8b8dfd36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce10dce4-5dfe-41d7-906b-15f120265b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"feedback-prize-english-language-learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba658c-c843-4dc2-82dc-9ca920ae4d3e",
   "metadata": {},
   "source": [
    "Load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ae2a4a-4407-4a3b-9301-a65fcea7d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss = pd.read_csv(path/\"sample_submission.csv\")\n",
    "df_tst = pd.read_csv(path/\"test.csv\")\n",
    "df_trn = pd.read_csv(path/\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466112f7-4d60-4c40-84fe-cebddf07bb72",
   "metadata": {},
   "source": [
    "We want to use the small DistilBERT model as we care about our enviroment üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c733384a-4742-43ee-b895-41ece9fe0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe58b5c-fb94-4058-ade6-9303a6429d95",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dab271-52a9-4142-a788-1a7c1e90b0f0",
   "metadata": {},
   "source": [
    "Setting our labels and hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91a4d986-db25-43f9-bea7-dab9a301e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_trn.columns[2:]\n",
    "df_trn_ohe = pd.get_dummies(df_trn, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf82526-99ca-46df-b908-eda408a58b3b",
   "metadata": {},
   "source": [
    "Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "037edb05-7030-43a1-b2b7-50ad58cdbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df_trn_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34579888-6a05-443d-8649-fc16b124c01b",
   "metadata": {},
   "source": [
    "Split in training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43e65f6e-d8ba-4aee-91b1-5109c0f8a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_d = ds.train_test_split(0.25, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb112478-e054-4e48-83e9-d642b40993dd",
   "metadata": {},
   "source": [
    "To identify dataset labels at a later point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aee49226-16c6-402c-912d-9688c22e0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label in ds_d['train'].features.keys() if label not in ['text_id', 'full_text']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29840adb-190e-44e0-8601-e3d8c9e08358",
   "metadata": {},
   "source": [
    "Preprocess the data with the help of our tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de315fe3-0e5d-4a06-a843-c8485bdd15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"full_text\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44fa6a3-981e-48ac-a561-18f0216acfd9",
   "metadata": {},
   "source": [
    "Encode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffd4306b-5ff8-49fa-86dc-b3bb863975e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d945d9a504e4491b19eea946d49170d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8290eb34a576473fb4362439305a2b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc_ds_d = ds_d.map(preprocess_data, batched=True, remove_columns=ds_d['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edac67-712e-4256-a1d4-4b17d6896d84",
   "metadata": {},
   "source": [
    "## Getting ready for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f32f4-27f3-4850-91d8-6ccbce72a333",
   "metadata": {},
   "source": [
    "Our metric, mcrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4924573-3f7f-4152-9a62-d7288cb33f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcrmse(y_true,y_pred): return np.mean(np.sqrt(np.mean((y_true-y_pred)**2,axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c5125-d312-4952-be64-5b595ac48123",
   "metadata": {},
   "source": [
    "See [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "514357fc-77ba-435e-9272-187fadcf81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    mcrmse_acc = mcrmse(y_true=y_true, y_pred=y_pred)\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)    \n",
    "    # return as dictionary\n",
    "    metrics = {'mcrmse': mcrmse_acc,\n",
    "               'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcbdde5-0d84-48af-9b5f-1695b63654e6",
   "metadata": {},
   "source": [
    "Setting up our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a702ad85-6725-4d62-91bb-cfd7aebc9ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"cohesion_1.0\",\n",
      "    \"1\": \"cohesion_1.5\",\n",
      "    \"2\": \"cohesion_2.0\",\n",
      "    \"3\": \"cohesion_2.5\",\n",
      "    \"4\": \"cohesion_3.0\",\n",
      "    \"5\": \"cohesion_3.5\",\n",
      "    \"6\": \"cohesion_4.0\",\n",
      "    \"7\": \"cohesion_4.5\",\n",
      "    \"8\": \"cohesion_5.0\",\n",
      "    \"9\": \"syntax_1.0\",\n",
      "    \"10\": \"syntax_1.5\",\n",
      "    \"11\": \"syntax_2.0\",\n",
      "    \"12\": \"syntax_2.5\",\n",
      "    \"13\": \"syntax_3.0\",\n",
      "    \"14\": \"syntax_3.5\",\n",
      "    \"15\": \"syntax_4.0\",\n",
      "    \"16\": \"syntax_4.5\",\n",
      "    \"17\": \"syntax_5.0\",\n",
      "    \"18\": \"vocabulary_1.0\",\n",
      "    \"19\": \"vocabulary_1.5\",\n",
      "    \"20\": \"vocabulary_2.0\",\n",
      "    \"21\": \"vocabulary_2.5\",\n",
      "    \"22\": \"vocabulary_3.0\",\n",
      "    \"23\": \"vocabulary_3.5\",\n",
      "    \"24\": \"vocabulary_4.0\",\n",
      "    \"25\": \"vocabulary_4.5\",\n",
      "    \"26\": \"vocabulary_5.0\",\n",
      "    \"27\": \"phraseology_1.0\",\n",
      "    \"28\": \"phraseology_1.5\",\n",
      "    \"29\": \"phraseology_2.0\",\n",
      "    \"30\": \"phraseology_2.5\",\n",
      "    \"31\": \"phraseology_3.0\",\n",
      "    \"32\": \"phraseology_3.5\",\n",
      "    \"33\": \"phraseology_4.0\",\n",
      "    \"34\": \"phraseology_4.5\",\n",
      "    \"35\": \"phraseology_5.0\",\n",
      "    \"36\": \"grammar_1.0\",\n",
      "    \"37\": \"grammar_1.5\",\n",
      "    \"38\": \"grammar_2.0\",\n",
      "    \"39\": \"grammar_2.5\",\n",
      "    \"40\": \"grammar_3.0\",\n",
      "    \"41\": \"grammar_3.5\",\n",
      "    \"42\": \"grammar_4.0\",\n",
      "    \"43\": \"grammar_4.5\",\n",
      "    \"44\": \"grammar_5.0\",\n",
      "    \"45\": \"conventions_1.0\",\n",
      "    \"46\": \"conventions_1.5\",\n",
      "    \"47\": \"conventions_2.0\",\n",
      "    \"48\": \"conventions_2.5\",\n",
      "    \"49\": \"conventions_3.0\",\n",
      "    \"50\": \"conventions_3.5\",\n",
      "    \"51\": \"conventions_4.0\",\n",
      "    \"52\": \"conventions_4.5\",\n",
      "    \"53\": \"conventions_5.0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"cohesion_1.0\": 0,\n",
      "    \"cohesion_1.5\": 1,\n",
      "    \"cohesion_2.0\": 2,\n",
      "    \"cohesion_2.5\": 3,\n",
      "    \"cohesion_3.0\": 4,\n",
      "    \"cohesion_3.5\": 5,\n",
      "    \"cohesion_4.0\": 6,\n",
      "    \"cohesion_4.5\": 7,\n",
      "    \"cohesion_5.0\": 8,\n",
      "    \"conventions_1.0\": 45,\n",
      "    \"conventions_1.5\": 46,\n",
      "    \"conventions_2.0\": 47,\n",
      "    \"conventions_2.5\": 48,\n",
      "    \"conventions_3.0\": 49,\n",
      "    \"conventions_3.5\": 50,\n",
      "    \"conventions_4.0\": 51,\n",
      "    \"conventions_4.5\": 52,\n",
      "    \"conventions_5.0\": 53,\n",
      "    \"grammar_1.0\": 36,\n",
      "    \"grammar_1.5\": 37,\n",
      "    \"grammar_2.0\": 38,\n",
      "    \"grammar_2.5\": 39,\n",
      "    \"grammar_3.0\": 40,\n",
      "    \"grammar_3.5\": 41,\n",
      "    \"grammar_4.0\": 42,\n",
      "    \"grammar_4.5\": 43,\n",
      "    \"grammar_5.0\": 44,\n",
      "    \"phraseology_1.0\": 27,\n",
      "    \"phraseology_1.5\": 28,\n",
      "    \"phraseology_2.0\": 29,\n",
      "    \"phraseology_2.5\": 30,\n",
      "    \"phraseology_3.0\": 31,\n",
      "    \"phraseology_3.5\": 32,\n",
      "    \"phraseology_4.0\": 33,\n",
      "    \"phraseology_4.5\": 34,\n",
      "    \"phraseology_5.0\": 35,\n",
      "    \"syntax_1.0\": 9,\n",
      "    \"syntax_1.5\": 10,\n",
      "    \"syntax_2.0\": 11,\n",
      "    \"syntax_2.5\": 12,\n",
      "    \"syntax_3.0\": 13,\n",
      "    \"syntax_3.5\": 14,\n",
      "    \"syntax_4.0\": 15,\n",
      "    \"syntax_4.5\": 16,\n",
      "    \"syntax_5.0\": 17,\n",
      "    \"vocabulary_1.0\": 18,\n",
      "    \"vocabulary_1.5\": 19,\n",
      "    \"vocabulary_2.0\": 20,\n",
      "    \"vocabulary_2.5\": 21,\n",
      "    \"vocabulary_3.0\": 22,\n",
      "    \"vocabulary_3.5\": 23,\n",
      "    \"vocabulary_4.0\": 24,\n",
      "    \"vocabulary_4.5\": 25,\n",
      "    \"vocabulary_5.0\": 26\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e1edd-3c30-40ca-aaf7-8a60e8d63c83",
   "metadata": {},
   "source": [
    "Hyperparameters & arguments setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2077767e-a9ed-4c9a-b5a3-9f62e06faff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "metric_for_best_model = \"mcrmse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1f1b54b-cfe2-4daf-8cef-b997f85b7627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"outputs\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=8e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92778b1e-19f1-4b04-844c-3c49bc301bdd",
   "metadata": {},
   "source": [
    "Set format to torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2f9511e-7b93-4ef7-bcdc-e05384391806",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_ds_d.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543fecb4-dcf3-4fb4-a890-e1ffd64e49b7",
   "metadata": {},
   "source": [
    "Check for correct batch format and forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "efad1604-fe4d-49f5-9205-400040885096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_ds_d['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6714edb-1520-49b7-987f-856749e23bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2722,  1110,  1141,  1104,  1103,  1211,  2712,  1282,  1107,\n",
       "         1142,  1362,   119,  2722,  1144,  7424,  1104,  4333,   117,  1152,\n",
       "         1138,  1992,  5862,  3514,  1176,  5230,  6331,   117,  7120,   117,\n",
       "         6244,  1186,  6331,   117,  1105,  1242,  1167,   119,  1220,  1138,\n",
       "         1992,  4706,  1116,  1115,  2006,  1166,  1620,   117,  1288,  1234,\n",
       "          117,  1152,  1256,  1138,  1199,  3505,  1956,  1176,  1152,  1138,\n",
       "         1138,  1832,  5082,  7418,  1656,  1103,  4706,   117,  1141,  1104,\n",
       "         1172,  1144,   170,  4255,  1656,   119,  2857,   146,  1156,  6657,\n",
       "         1120,  2722,   146,  1156,  1567,  1106,  2222,  1147,  2094,  1272,\n",
       "          146,  1767,  1147,  2094,  1108,  1363,  1177,  1358,   146, 14516,\n",
       "         1116, 10340,  1174,  1122,  1146,  1105,  1103,  2094,  2736,  1177,\n",
       "          194,  1818,  4527,  1105, 27629, 13913,   119,   146,  1156,  1145,\n",
       "         1567,  1106,  4176,  1113,   170,  3499,  1105, 11270,  1194,  1103,\n",
       "         1331,   119,  1448,  1104,  1103,  1363,  1614,  1110,  1115,  1122,\n",
       "          112,   188,  2633,  1211,  1104,  1159,  1166,  1175,   119,  7120,\n",
       "         1110,   170,  2712,  1331,  1107,  2722,   117,  1152,  2936, 14694,\n",
       "          119,  2082,  1104,  2124,  1234,  1156,  1178,  2936,  2124,  1133,\n",
       "         1114,  1126,  9603,  1133,  7120,  8917,  2124,  1105, 14694,   119,\n",
       "         1337,  1156,  1141,  1103,  3483,   178,  1156,  1567,  1106,  3858,\n",
       "          119,  2722,  1144,  2393,  3329,  1104,  1632,  3038,  1176,  9066,\n",
       "         4907,   168,  1392,   117,  9066,  4907,   168,  1392,   117,  9066,\n",
       "         4907,   168,  1392,   117,  1105,   170,  2006,  9670,  1167,   119,\n",
       "         1220,  1296,  1138,   170,  4706,   119,  1109,  1314,  1282,   146,\n",
       "         1156,  1567,  1106,  3143,  1110,  1103,  2364,  1104,  2722,  1134,\n",
       "         1110,  1363,   119,  9066,  4907,   168,  1392,  1110,  1141,  1104,\n",
       "         1103,  1436,  5862,  1264,  1107,  1103,  1362,   119,  1220,  1138,\n",
       "          170,  1974,  1104,  1632,  2139,  1105,   170,  2562,  4706,   119,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_ds_d['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5802d525-4e34-48f4-abb4-352fab42b8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6894, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.0697, -0.1150,  0.1347, -0.0884, -0.0258, -0.0756,  0.0942,  0.0280,\n",
       "         -0.0434, -0.0496,  0.1531, -0.0448,  0.0982, -0.0008, -0.0036, -0.0964,\n",
       "         -0.1115, -0.1378, -0.0158,  0.1075,  0.0232,  0.0487,  0.0406,  0.1637,\n",
       "          0.0174,  0.0351, -0.0200,  0.0281, -0.0097, -0.0534,  0.0546, -0.0334,\n",
       "         -0.1012,  0.0192, -0.1326,  0.0234, -0.0674,  0.0868,  0.0536, -0.0165,\n",
       "          0.0937,  0.0105,  0.1397, -0.1875,  0.0375,  0.0325, -0.0408, -0.0414,\n",
       "          0.0562, -0.0262, -0.0540, -0.0930,  0.0205, -0.0305]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=enc_ds_d['train']['input_ids'][0].unsqueeze(0), labels=enc_ds_d['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402a586-9b35-48cf-bed9-be2a0992e746",
   "metadata": {},
   "source": [
    "Setting up our Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c47fb647-b3e4-4e4c-9049-dbc6540f3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=enc_ds_d[\"train\"],\n",
    "    eval_dataset=enc_ds_d[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8695d93-eaac-4ddf-9d08-636d58e63db8",
   "metadata": {},
   "source": [
    "**TRAIN!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15286cd6-68aa-4794-8dcd-e307d51b62f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2933\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 13:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcrmse</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.285174</td>\n",
       "      <td>0.276595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.277689</td>\n",
       "      <td>0.276595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.264235</td>\n",
       "      <td>0.277174</td>\n",
       "      <td>0.053526</td>\n",
       "      <td>0.512046</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.263474</td>\n",
       "      <td>0.276842</td>\n",
       "      <td>0.041069</td>\n",
       "      <td>0.509224</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.261615</td>\n",
       "      <td>0.276930</td>\n",
       "      <td>0.058494</td>\n",
       "      <td>0.513399</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 978\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to outputs/checkpoint-184\n",
      "Configuration saved in outputs/checkpoint-184/config.json\n",
      "Model weights saved in outputs/checkpoint-184/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-184/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-184/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 978\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to outputs/checkpoint-368\n",
      "Configuration saved in outputs/checkpoint-368/config.json\n",
      "Model weights saved in outputs/checkpoint-368/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-368/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-368/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 978\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to outputs/checkpoint-552\n",
      "Configuration saved in outputs/checkpoint-552/config.json\n",
      "Model weights saved in outputs/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-552/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 978\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to outputs/checkpoint-736\n",
      "Configuration saved in outputs/checkpoint-736/config.json\n",
      "Model weights saved in outputs/checkpoint-736/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-736/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-736/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 978\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to outputs/checkpoint-920\n",
      "Configuration saved in outputs/checkpoint-920/config.json\n",
      "Model weights saved in outputs/checkpoint-920/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-920/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-920/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/checkpoint-552 (score: 0.2771742588424308).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=920, training_loss=0.2778122279954993, metrics={'train_runtime': 794.4716, 'train_samples_per_second': 18.459, 'train_steps_per_second': 1.158, 'total_flos': 1944435895879680.0, 'train_loss': 0.2778122279954993, 'epoch': 5.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b02058c-0db5-4d3e-bb51-dcc7655d5232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trainer\n",
      "Configuration saved in trainer/config.json\n",
      "Model weights saved in trainer/pytorch_model.bin\n",
      "tokenizer config file saved in trainer/tokenizer_config.json\n",
      "Special tokens file saved in trainer/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57f9dd-8551-47ff-9261-136404f622f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
