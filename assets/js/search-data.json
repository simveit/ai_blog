{
  
    
        "post0": {
            "title": "Inference with ü§ó",
            "content": "In this notebook I showed how to build a simple model in huggingface. . Other from that I just used trainer.save_model(&quot;trainer&quot;) to save my model in the folder trainer. . From there i just need to exchange the loading of model file in the notebook and setup the trainer for inference mode as described in here. Everything else is pretty much the same üòä . from datasets import Dataset,DatasetDict import matplotlib.pyplot as plt import numpy as np import pandas as pd from pathlib import Path import seaborn as sns from transformers import AutoModelForSequenceClassification,AutoTokenizer import torch # Set nice style plt.style.use([&#39;dark_background&#39;]) . Setup . path = Path(&quot;feedback-prize-english-language-learning&quot;) . Get an overview of what is contained in our folder. . !ls {path} . sample_submission.csv test.csv train.csv . df_ss = pd.read_csv(path/&quot;sample_submission.csv&quot;) df_tst = pd.read_csv(path/&quot;test.csv&quot;) df_trn = pd.read_csv(path/&quot;train.csv&quot;) . labels = df_trn.columns[2:] . Preprocess . To use the transformers we need a dataset. . Let&#39;s initialize our tokenizer: . model_nm = &#39;trainer&#39; . tokz = AutoTokenizer.from_pretrained(model_nm) . Let&#39;s see how exactly this works: . tokz.tokenize(&quot;I want to get tokenized!&quot;) . [&#39;‚ñÅI&#39;, &#39;‚ñÅwant&#39;, &#39;‚ñÅto&#39;, &#39;‚ñÅget&#39;, &#39;‚ñÅtoken&#39;, &#39;ized&#39;, &#39;!&#39;] . We can encode... . enc = tokz.encode((&quot;I want to get tokenized!&quot;)) enc . [1, 273, 409, 264, 350, 10704, 4666, 300, 2] . ... and decode . tokz.decode(enc) . &#39;[CLS] I want to get tokenized![SEP]&#39; . Now we need to do 2 things: . tokenize our data, that means transforming the text into some form the computer can process | initialize our multi label for the model. | . Let&#39;s first tackle the problem of generating categories: . We One hot encode | We use the resulting df to generate our dataset! | . tempdf = pd.get_dummies(df_trn, columns=labels) . ds = Dataset.from_pandas(tempdf) . Split in training and validation! . ds_d = ds.train_test_split(0.25, seed=42) . Mapping between labels and integers . labels = [label for label in ds_d[&#39;train&#39;].features.keys() if label not in [&#39;text_id&#39;, &#39;full_text&#39;]] id2label = {idx:label for idx, label in enumerate(labels)} label2id = {label:idx for idx, label in enumerate(labels)} labels[:5] . [&#39;cohesion_1.0&#39;, &#39;cohesion_1.5&#39;, &#39;cohesion_2.0&#39;, &#39;cohesion_2.5&#39;, &#39;cohesion_3.0&#39;] . def preprocess_data(examples): # take a batch of texts text = examples[&quot;full_text&quot;] # encode them encoding = tokz(text, padding=&quot;max_length&quot;, truncation=True, max_length=128) # add labels labels_batch = {k: examples[k] for k in examples.keys() if k in labels} # create numpy array of shape (batch_size, num_labels) labels_matrix = np.zeros((len(text), len(labels))) # fill numpy array for idx, label in enumerate(labels): labels_matrix[:, idx] = labels_batch[label] encoding[&quot;labels&quot;] = labels_matrix.tolist() return encoding . enc_ds_d = ds_d.map(preprocess_data, batched=True, remove_columns=ds_d[&#39;train&#39;].column_names) . Parameter &#39;function&#39;=&lt;function preprocess_data at 0x7fe018278d30&gt; of the transform datasets.arrow_dataset.Dataset._map_single couldn&#39;t be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won&#39;t be showed. . enc_ds_d . DatasetDict({ train: Dataset({ features: [&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;], num_rows: 2933 }) test: Dataset({ features: [&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;], num_rows: 978 }) }) . Split in train and validation set . Setting up the model &amp; trainer . Setting up the metric... . def mcrmse(x,y): return np.mean(np.sqrt(np.mean((x-y)**2,axis=0))) . x1 = np.array([[1,1],[0,0]]) x2 = np.array([[1,0],[0,1]]) . mcrmse(x1,x2) . 0.5 . def mcrmse_d(eval_pred): return {&#39;mcrmse&#39;: mcrmse(*eval_pred)} . from transformers import TrainingArguments,Trainer . ...Batch size and metric name... . batch_size = 8 metric_name = &quot;mcrmse&quot; . ...the arguments... . args = TrainingArguments( f&quot;tst_out&quot;, do_train = False, do_predict = True, per_device_eval_batch_size = batch_size, dataloader_drop_last = False ) . from sklearn.metrics import f1_score, roc_auc_score, accuracy_score from transformers import EvalPrediction import torch # source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/ def multi_label_metrics(predictions, labels, threshold=0.5): # first, apply sigmoid on predictions which are of shape (batch_size, num_labels) sigmoid = torch.nn.Sigmoid() probs = sigmoid(torch.Tensor(predictions)) # next, use threshold to turn them into integer predictions y_pred = np.zeros(probs.shape) y_pred[np.where(probs &gt;= threshold)] = 1 # finally, compute metrics y_true = labels mcrmse_acc = mcrmse(y_true, y_pred) # return as dictionary metrics = {&#39;mcrmse&#39;: mcrmse_acc} return metrics def compute_metrics(p: EvalPrediction): preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions result = multi_label_metrics( predictions=preds, labels=p.label_ids) return result . ...the model... . from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(model_nm, problem_type=&quot;multi_label_classification&quot;, num_labels=len(labels), id2label=id2label, label2id=label2id) . enc_ds_d.set_format(&quot;torch&quot;) . Let&#39;s verify a batch as well as a forward pass: . enc_ds_d[&#39;train&#39;][0][&#39;labels&#39;].type() . &#39;torch.FloatTensor&#39; . enc_ds_d[&#39;train&#39;][&#39;input_ids&#39;][0] . tensor([ 1, 4136, 269, 311, 265, 262, 370, 874, 470, 267, 291, 447, 260, 4136, 303, 1738, 265, 1451, 261, 306, 286, 610, 6135, 4986, 334, 2858, 8202, 261, 7934, 261, 13381, 795, 8202, 261, 263, 386, 310, 260, 450, 286, 610, 34772, 272, 783, 360, 803, 261, 528, 355, 261, 306, 402, 286, 347, 1085, 830, 334, 306, 286, 286, 1090, 8926, 16224, 1013, 262, 8425, 261, 311, 265, 349, 303, 266, 2553, 1013, 260, 1414, 273, 338, 3753, 288, 4136, 273, 338, 472, 264, 687, 308, 645, 401, 273, 1331, 308, 645, 284, 397, 324, 1964, 273, 40756, 834, 31452, 278, 322, 263, 262, 645, 1127, 324, 12516, 263, 7741, 260, 273, 338, 327, 472, 264, 2224, 277, 266, 2750, 263, 5771, 390, 262, 707, 260, 2]) . outputs = model(input_ids=enc_ds_d[&#39;train&#39;][&#39;input_ids&#39;][0].unsqueeze(0), labels=enc_ds_d[&#39;train&#39;][0][&#39;labels&#39;].unsqueeze(0)) outputs . SequenceClassifierOutput(loss=tensor(0.1572, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward0&gt;), logits=tensor([[-7.6003, -6.2087, -1.6748, -0.1617, -0.7942, -2.9857, -5.6274, -7.9416, -8.3018, -7.3899, -5.6492, -1.3790, 0.0663, -1.1804, -4.2006, -6.4937, -8.1050, -8.4808, -8.3238, -6.9928, -3.2913, -0.7201, 0.4488, -3.5915, -5.5989, -7.6971, -8.4161, -6.8264, -7.3330, -1.5203, -0.0205, -0.8572, -3.8849, -5.9465, -8.1325, -8.1105, -7.3505, -6.5058, -0.8089, -0.1246, -1.5866, -4.0718, -5.6008, -7.9227, -8.0366, -6.8729, -6.0449, -1.1963, 0.0729, -1.3299, -3.5036, -5.9796, -7.8256, -8.9868]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None) . trainer = Trainer( model, args, train_dataset=enc_ds_d[&quot;train&quot;], eval_dataset=enc_ds_d[&quot;test&quot;], tokenizer=tokz, compute_metrics=compute_metrics ) . Predict . Set up dataset . def tok_func(x): return tokz(x[&quot;full_text&quot;]) . ds_tst = Dataset.from_pandas(df_tst) . eval_ds = ds_tst.map(tok_func, batched=True, remove_columns=[&quot;text_id&quot;,&quot;full_text&quot;]) . preds = trainer.predict(eval_ds) . ***** Running Prediction ***** Num examples = 3 Batch size = 8 . . [1/1 : &lt; :] preds_tt = torch.Tensor(preds.predictions) . sigmoid = torch.nn.Sigmoid() probs = sigmoid(preds_tt) . probs.shape . torch.Size([3, 54]) . probs_cat = torch.reshape(probs,(eval_ds.num_rows,6,-1)) probs_cat . tensor([[[4.3149e-04, 1.6425e-03, 1.4542e-01, 4.5501e-01, 3.2861e-01, 5.1657e-02, 3.7629e-03, 3.3773e-04, 2.2183e-04], [5.0841e-04, 2.9085e-03, 1.8654e-01, 5.1705e-01, 2.5370e-01, 1.5948e-02, 1.6301e-03, 2.7220e-04, 1.8518e-04], [2.0637e-04, 7.6860e-04, 3.1243e-02, 3.1676e-01, 6.1975e-01, 2.9120e-02, 3.9431e-03, 4.3889e-04, 2.0195e-04], [9.2469e-04, 5.5310e-04, 1.6003e-01, 4.7628e-01, 3.1655e-01, 2.2838e-02, 2.7699e-03, 2.7647e-04, 2.7412e-04], [5.4792e-04, 1.2285e-03, 2.7858e-01, 4.6222e-01, 1.8477e-01, 1.9232e-02, 4.0661e-03, 3.4774e-04, 3.0245e-04], [8.6861e-04, 1.9523e-03, 2.1894e-01, 5.2350e-01, 2.2068e-01, 3.2152e-02, 2.5723e-03, 3.9099e-04, 1.1256e-04]], [[9.6675e-04, 3.8596e-03, 2.3323e-01, 4.9773e-01, 2.5149e-01, 3.2815e-02, 2.7687e-03, 3.8969e-04, 2.9928e-04], [1.2245e-03, 6.5233e-03, 3.1888e-01, 5.1605e-01, 1.5547e-01, 1.0238e-02, 1.2655e-03, 3.7664e-04, 2.6395e-04], [3.9965e-04, 1.5746e-03, 6.1339e-02, 3.8032e-01, 5.5359e-01, 1.7535e-02, 2.4546e-03, 5.0636e-04, 2.7842e-04], [1.7846e-03, 1.1604e-03, 2.7300e-01, 5.2861e-01, 2.2545e-01, 1.3480e-02, 1.9217e-03, 3.3775e-04, 3.9268e-04], [1.1056e-03, 2.7692e-03, 4.4649e-01, 4.6234e-01, 1.1940e-01, 9.3059e-03, 2.6974e-03, 4.4980e-04, 3.8357e-04], [1.9633e-03, 4.2692e-03, 3.0920e-01, 5.1076e-01, 1.6183e-01, 2.1647e-02, 2.1585e-03, 4.6869e-04, 1.6160e-04]], [[5.6670e-05, 1.4889e-04, 2.9396e-03, 4.3957e-02, 2.8636e-01, 5.2281e-01, 1.8473e-01, 4.0403e-03, 9.8352e-04], [1.1397e-04, 1.3573e-04, 3.2018e-03, 2.5938e-02, 4.4911e-01, 4.8044e-01, 9.8053e-02, 2.3610e-03, 5.8633e-04], [8.8751e-05, 1.3494e-04, 6.8702e-04, 1.0259e-02, 2.5660e-01, 5.0850e-01, 2.0871e-01, 5.2785e-03, 1.5634e-03], [1.7101e-04, 5.5315e-05, 2.6744e-03, 2.1324e-02, 3.1220e-01, 5.1904e-01, 1.6054e-01, 5.1044e-03, 7.5152e-04], [1.3115e-04, 1.1017e-04, 4.2908e-03, 4.7419e-02, 3.0202e-01, 4.8949e-01, 1.5539e-01, 5.1233e-03, 1.3146e-03], [1.3011e-04, 2.1449e-04, 2.4913e-03, 2.9781e-02, 3.6902e-01, 4.5032e-01, 1.3317e-01, 5.5649e-03, 5.4810e-04]]]) . pred = torch.argmax(probs_cat, axis=2) pred . tensor([[3, 3, 4, 3, 3, 3], [3, 3, 4, 3, 3, 3], [5, 5, 5, 5, 5, 5]]) . If we get prediction for the i&#39;th column that means we have have for the category given by the row index . $grad_{cat} = i * 0.5 + 1$ . grades = pred*0.5+1 grades . tensor([[2.5000, 2.5000, 3.0000, 2.5000, 2.5000, 2.5000], [2.5000, 2.5000, 3.0000, 2.5000, 2.5000, 2.5000], [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]) . Let&#39;s convert this to a dataframe: . sub = pd.DataFrame(grades.numpy()) sub.columns = df_ss.columns[1:] sub[&quot;text_id&quot;] = ds_tst[&quot;text_id&quot;] sub . cohesion syntax vocabulary phraseology grammar conventions text_id . 0 2.5 | 2.5 | 3.0 | 2.5 | 2.5 | 2.5 | 0000C359D63E | . 1 2.5 | 2.5 | 3.0 | 2.5 | 2.5 | 2.5 | 000BAD50D026 | . 2 3.5 | 3.5 | 3.5 | 3.5 | 3.5 | 3.5 | 00367BB2546B | . df_ss . text_id cohesion syntax vocabulary phraseology grammar conventions . 0 0000C359D63E | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . 1 000BAD50D026 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . 2 00367BB2546B | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . We need to rearrange the columns to get the right format. . sub = sub[df_ss.columns] sub . text_id cohesion syntax vocabulary phraseology grammar conventions . 0 0000C359D63E | 2.5 | 2.5 | 3.0 | 2.5 | 2.5 | 2.5 | . 1 000BAD50D026 | 2.5 | 2.5 | 3.0 | 2.5 | 2.5 | 2.5 | . 2 00367BB2546B | 3.5 | 3.5 | 3.5 | 3.5 | 3.5 | 3.5 | .",
            "url": "https://simveit.github.io/ai_blog/2022/09/06/First-huggingface-inference.html",
            "relUrl": "/2022/09/06/First-huggingface-inference.html",
            "date": " ‚Ä¢ Sep 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "A first ü§ó model",
            "content": "As Lesson 4 of the fastai DL course deals with NLP, I decided to give this competition on kaggle a try. . Many hints to how implement multi label classification can be found in the following great notebook_for_multi_label_text_classification.ipynb) . This is my first NLP model, so please make sure I didn&#39;t make any mistakes and if so, pls let me know üòÄ . Have fun reading ‚ò∫Ô∏è . from datasets import Dataset,DatasetDict import matplotlib.pyplot as plt import numpy as np import pandas as pd from pathlib import Path import seaborn as sns from transformers import AutoModelForSequenceClassification,AutoTokenizer import torch # Set nice style plt.style.use([&#39;dark_background&#39;]) . A first look at our data . path = Path(&quot;feedback-prize-english-language-learning&quot;) . Get an overview of what is contained in our folder. . !ls {path} . sample_submission.csv test.csv train.csv . df_ss = pd.read_csv(path/&quot;sample_submission.csv&quot;) df_tst = pd.read_csv(path/&quot;test.csv&quot;) df_trn = pd.read_csv(path/&quot;train.csv&quot;) . From the competition site we know that we have six different categories which determine the proficiency in essay writing of a given student. Higher scores -&gt; Higher proficiency. . df_ss.head() . text_id cohesion syntax vocabulary phraseology grammar conventions . 0 0000C359D63E | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . 1 000BAD50D026 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . 2 00367BB2546B | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . Let&#39;s look at example of a training data. . We have for each text_id a text and the corresponding grades: . df_trn.head(1).T . 0 . text_id 0016926B079C | . full_text I think that students would benefit from learn... | . cohesion 3.5 | . syntax 3.5 | . vocabulary 3.0 | . phraseology 3.0 | . grammar 4.0 | . conventions 3.0 | . Example of a text: . df_trn.iloc[0].full_text . &#34;I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they&#39;ll be pay more attention. they will be comfortable at home. n nThe hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you&#39;ll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear. n nmost students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go. n nwhen your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class. &#34; . Let&#39;s look at the distribution of the grades: . Let&#39;s take a look of the distribution of grades: . labels = df_trn.columns[2:] nrow = 3 ncol = 2 fig, axs = plt.subplots(nrow, ncol, tight_layout=True, figsize = (12,8)) for i, ax in enumerate(fig.axes): ax.hist(df_trn[labels[i]], bins=8); ax.set_title(labels[i]) . We can see that there are many medium grades with just a few outliers on the left spectrum. . We should probably take this into account when building our training and validation set so that our model is also able to grade a bad student. . Let&#39;s look at the rough numbers summarized in a pretty looking data frame: . df_trn[labels].apply(pd.value_counts) . cohesion syntax vocabulary phraseology grammar conventions . 1.0 10 | 11 | 2 | 10 | 8 | 15 | . 1.5 27 | 29 | 14 | 11 | 20 | 20 | . 2.0 315 | 410 | 124 | 350 | 544 | 402 | . 2.5 790 | 839 | 528 | 772 | 855 | 784 | . 3.0 1096 | 1250 | 1503 | 1153 | 994 | 1151 | . 3.5 988 | 867 | 1007 | 929 | 880 | 908 | . 4.0 534 | 388 | 577 | 553 | 447 | 484 | . 4.5 125 | 100 | 115 | 108 | 134 | 122 | . 5.0 26 | 17 | 41 | 25 | 29 | 25 | . Especially of the vocabulary and phraseology we should take care that we sample enough examples from the $ leq 1.0$ range. . Another point of interest is the words contained in our texts. Let&#39;s check that we don&#39;t have extreme outliers as Transformers don&#39;t work good for long text inputs: . df_trn[&quot;full_text&quot;].apply(lambda x: len(x.split())).hist(); . We see that most texts are made of 200-600 words. . Preprocess . To use the transformers we need a dataset. . ds = Dataset.from_pandas(df_trn) . ds . Dataset({ features: [&#39;text_id&#39;, &#39;full_text&#39;, &#39;cohesion&#39;, &#39;syntax&#39;, &#39;vocabulary&#39;, &#39;phraseology&#39;, &#39;grammar&#39;, &#39;conventions&#39;], num_rows: 3911 }) . What exactly is a dataset? . Let&#39;s explore. . We have an Attribut column_name . ds.column_names . [&#39;text_id&#39;, &#39;full_text&#39;, &#39;cohesion&#39;, &#39;syntax&#39;, &#39;vocabulary&#39;, &#39;phraseology&#39;, &#39;grammar&#39;, &#39;conventions&#39;] . With ds[i]we can get the i&#39;th element . ds[0] . {&#39;text_id&#39;: &#39;0016926B079C&#39;, &#39;full_text&#39;: &#34;I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they&#39;ll be pay more attention. they will be comfortable at home. n nThe hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you&#39;ll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear. n nmost students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go. n nwhen your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class. &#34;, &#39;cohesion&#39;: 3.5, &#39;syntax&#39;: 3.5, &#39;vocabulary&#39;: 3.0, &#39;phraseology&#39;: 3.0, &#39;grammar&#39;: 4.0, &#39;conventions&#39;: 3.0} . Let&#39;s initialize our tokenizer: . model_nm = &#39;microsoft/deberta-v3-small&#39; . tokz = AutoTokenizer.from_pretrained(model_nm) . Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. /usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text. warnings.warn( Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. . Let&#39;s see how exactly this works: . tokz.tokenize(&quot;I want to get tokenized!&quot;) . [&#39;‚ñÅI&#39;, &#39;‚ñÅwant&#39;, &#39;‚ñÅto&#39;, &#39;‚ñÅget&#39;, &#39;‚ñÅtoken&#39;, &#39;ized&#39;, &#39;!&#39;] . We can encode... . enc = tokz.encode((&quot;I want to get tokenized!&quot;)) enc . [1, 273, 409, 264, 350, 10704, 4666, 300, 2] . ... and decode . tokz.decode(enc) . &#39;[CLS] I want to get tokenized![SEP]&#39; . Now we need to do 2 things: . tokenize our data, that means transforming the text into some form the computer can process | initialize our multi label for the model. | . Let&#39;s first tackle the problem of generating categories: . We One hot encode | We use the resulting df to generate our dataset! | . tempdf = pd.get_dummies(df_trn, columns=labels) . ds = Dataset.from_pandas(tempdf) . Split in training and validation! . ds_d = ds.train_test_split(0.25, seed=42) . Mapping between labels and integers . labels = [label for label in ds_d[&#39;train&#39;].features.keys() if label not in [&#39;text_id&#39;, &#39;full_text&#39;]] id2label = {idx:label for idx, label in enumerate(labels)} label2id = {label:idx for idx, label in enumerate(labels)} labels[:5] . [&#39;cohesion_1.0&#39;, &#39;cohesion_1.5&#39;, &#39;cohesion_2.0&#39;, &#39;cohesion_2.5&#39;, &#39;cohesion_3.0&#39;] . def preprocess_data(examples): # take a batch of texts text = examples[&quot;full_text&quot;] # encode them encoding = tokz(text, padding=&quot;max_length&quot;, truncation=True, max_length=128) # add labels labels_batch = {k: examples[k] for k in examples.keys() if k in labels} # create numpy array of shape (batch_size, num_labels) labels_matrix = np.zeros((len(text), len(labels))) # fill numpy array for idx, label in enumerate(labels): labels_matrix[:, idx] = labels_batch[label] encoding[&quot;labels&quot;] = labels_matrix.tolist() return encoding . enc_ds_d = ds_d.map(preprocess_data, batched=True, remove_columns=ds_d[&#39;train&#39;].column_names) . Parameter &#39;function&#39;=&lt;function preprocess_data at 0x7fac4afecc10&gt; of the transform datasets.arrow_dataset.Dataset._map_single couldn&#39;t be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won&#39;t be showed. . enc_ds_d . DatasetDict({ train: Dataset({ features: [&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;], num_rows: 2933 }) test: Dataset({ features: [&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;], num_rows: 978 }) }) . Split in train and validation set . Training . Setting up the metric... . def mcrmse(x,y): return np.mean(np.sqrt(np.mean((x-y)**2,axis=0))) . x1 = np.array([[1,1],[0,0]]) x2 = np.array([[1,0],[0,1]]) . mcrmse(x1,x2) . 0.5 . def mcrmse_d(eval_pred): return {&#39;mcrmse&#39;: mcrmse(*eval_pred)} . from transformers import TrainingArguments,Trainer . ...Batch size and metric name... . batch_size = 8 metric_name = &quot;mcrmse&quot; . ...the arguments... . args = TrainingArguments( f&quot;outputs&quot;, evaluation_strategy = &quot;epoch&quot;, save_strategy = &quot;epoch&quot;, learning_rate=8e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, num_train_epochs=5, weight_decay=0.01, load_best_model_at_end=True, metric_for_best_model=metric_name, #push_to_hub=True, ) . from sklearn.metrics import f1_score, roc_auc_score, accuracy_score from transformers import EvalPrediction import torch # source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/ def multi_label_metrics(predictions, labels, threshold=0.5): # first, apply sigmoid on predictions which are of shape (batch_size, num_labels) sigmoid = torch.nn.Sigmoid() probs = sigmoid(torch.Tensor(predictions)) # next, use threshold to turn them into integer predictions y_pred = np.zeros(probs.shape) y_pred[np.where(probs &gt;= threshold)] = 1 # finally, compute metrics y_true = labels mcrmse_acc = mcrmse(y_true, y_pred) # return as dictionary metrics = {&#39;mcrmse&#39;: mcrmse_acc} return metrics def compute_metrics(p: EvalPrediction): preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions result = multi_label_metrics( predictions=preds, labels=p.label_ids) return result . ...the model... . from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(model_nm, problem_type=&quot;multi_label_classification&quot;, num_labels=len(labels), id2label=id2label, label2id=label2id) . Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: [&#39;mask_predictions.dense.bias&#39;, &#39;mask_predictions.LayerNorm.weight&#39;, &#39;mask_predictions.classifier.bias&#39;, &#39;lm_predictions.lm_head.bias&#39;, &#39;lm_predictions.lm_head.dense.bias&#39;, &#39;mask_predictions.dense.weight&#39;, &#39;lm_predictions.lm_head.LayerNorm.weight&#39;, &#39;lm_predictions.lm_head.dense.weight&#39;, &#39;lm_predictions.lm_head.LayerNorm.bias&#39;, &#39;mask_predictions.classifier.weight&#39;, &#39;mask_predictions.LayerNorm.bias&#39;] - This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: [&#39;classifier.weight&#39;, &#39;pooler.dense.bias&#39;, &#39;pooler.dense.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . TRAIN! . enc_ds_d.set_format(&quot;torch&quot;) . Let&#39;s verify a batch as well as a forward pass: . enc_ds_d[&#39;train&#39;][0][&#39;labels&#39;].type() . &#39;torch.FloatTensor&#39; . enc_ds_d[&#39;train&#39;][&#39;input_ids&#39;][0] . tensor([ 1, 4136, 269, 311, 265, 262, 370, 874, 470, 267, 291, 447, 260, 4136, 303, 1738, 265, 1451, 261, 306, 286, 610, 6135, 4986, 334, 2858, 8202, 261, 7934, 261, 13381, 795, 8202, 261, 263, 386, 310, 260, 450, 286, 610, 34772, 272, 783, 360, 803, 261, 528, 355, 261, 306, 402, 286, 347, 1085, 830, 334, 306, 286, 286, 1090, 8926, 16224, 1013, 262, 8425, 261, 311, 265, 349, 303, 266, 2553, 1013, 260, 1414, 273, 338, 3753, 288, 4136, 273, 338, 472, 264, 687, 308, 645, 401, 273, 1331, 308, 645, 284, 397, 324, 1964, 273, 40756, 834, 31452, 278, 322, 263, 262, 645, 1127, 324, 12516, 263, 7741, 260, 273, 338, 327, 472, 264, 2224, 277, 266, 2750, 263, 5771, 390, 262, 707, 260, 2]) . outputs = model(input_ids=enc_ds_d[&#39;train&#39;][&#39;input_ids&#39;][0].unsqueeze(0), labels=enc_ds_d[&#39;train&#39;][0][&#39;labels&#39;].unsqueeze(0)) outputs . SequenceClassifierOutput(loss=tensor(0.6910, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward0&gt;), logits=tensor([[ 0.1523, 0.1020, 0.0271, -0.0094, -0.0423, 0.1312, -0.1664, 0.2214, 0.1243, 0.0934, -0.0382, -0.1368, -0.1676, 0.0055, -0.2820, -0.0138, 0.1081, -0.1334, -0.1276, -0.0073, 0.0665, 0.3525, -0.2512, 0.1544, 0.1957, 0.2246, 0.1495, -0.0065, -0.1206, -0.0391, 0.0098, 0.0422, -0.1200, 0.1003, -0.0925, -0.0526, 0.0683, -0.2110, 0.0239, 0.1355, 0.2869, 0.0020, -0.1044, -0.4394, 0.2157, -0.1819, 0.0900, -0.0036, -0.0499, -0.2271, 0.0909, -0.1022, -0.1857, 0.1304]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None) . trainer = Trainer( model, args, train_dataset=enc_ds_d[&quot;train&quot;], eval_dataset=enc_ds_d[&quot;test&quot;], tokenizer=tokz, compute_metrics=compute_metrics ) . trainer.train() . /usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( ***** Running training ***** Num examples = 2933 Num Epochs = 5 Instantaneous batch size per device = 8 Total train batch size (w. parallel, distributed &amp; accumulation) = 8 Gradient Accumulation steps = 1 Total optimization steps = 1835 . . [1835/1835 06:54, Epoch 5/5] Epoch Training Loss Validation Loss Mcrmse . 1 | No log | 0.285186 | 0.276595 | . 2 | 0.294300 | 0.262396 | 0.277009 | . 3 | 0.256900 | 0.258025 | 0.276918 | . 4 | 0.256900 | 0.264292 | 0.277865 | . 5 | 0.237500 | 0.268947 | 0.280089 | . &lt;/div&gt; &lt;/div&gt; ***** Running Evaluation ***** Num examples = 978 Batch size = 8 Saving model checkpoint to outputs/checkpoint-367 Configuration saved in outputs/checkpoint-367/config.json Model weights saved in outputs/checkpoint-367/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-367/tokenizer_config.json Special tokens file saved in outputs/checkpoint-367/special_tokens_map.json ***** Running Evaluation ***** Num examples = 978 Batch size = 8 Saving model checkpoint to outputs/checkpoint-734 Configuration saved in outputs/checkpoint-734/config.json Model weights saved in outputs/checkpoint-734/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-734/tokenizer_config.json Special tokens file saved in outputs/checkpoint-734/special_tokens_map.json ***** Running Evaluation ***** Num examples = 978 Batch size = 8 Saving model checkpoint to outputs/checkpoint-1101 Configuration saved in outputs/checkpoint-1101/config.json Model weights saved in outputs/checkpoint-1101/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-1101/tokenizer_config.json Special tokens file saved in outputs/checkpoint-1101/special_tokens_map.json ***** Running Evaluation ***** Num examples = 978 Batch size = 8 Saving model checkpoint to outputs/checkpoint-1468 Configuration saved in outputs/checkpoint-1468/config.json Model weights saved in outputs/checkpoint-1468/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-1468/tokenizer_config.json Special tokens file saved in outputs/checkpoint-1468/special_tokens_map.json ***** Running Evaluation ***** Num examples = 978 Batch size = 8 Saving model checkpoint to outputs/checkpoint-1835 Configuration saved in outputs/checkpoint-1835/config.json Model weights saved in outputs/checkpoint-1835/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-1835/tokenizer_config.json Special tokens file saved in outputs/checkpoint-1835/special_tokens_map.json Training completed. Do not forget to share your model on huggingface.co/models =) Loading best model from outputs/checkpoint-1835 (score: 0.2800890973793673). . TrainOutput(global_step=1835, training_loss=0.2556868800025545, metrics={&#39;train_runtime&#39;: 414.8279, &#39;train_samples_per_second&#39;: 35.352, &#39;train_steps_per_second&#39;: 4.424, &#39;total_flos&#39;: 486126273507840.0, &#39;train_loss&#39;: 0.2556868800025545, &#39;epoch&#39;: 5.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Predict . Set up dataset . def tok_func(x): return tokz(x[&quot;full_text&quot;]) . ds_tst = Dataset.from_pandas(df_tst) . eval_ds = ds_tst.map(tok_func, batched=True, remove_columns=[&quot;text_id&quot;,&quot;full_text&quot;]) . An example for a text in the test set. . ds_tst[0][&quot;full_text&quot;][:30] . &#39;when a person has no experienc&#39; . text = ds_tst[0][&quot;full_text&quot;] encoding = tokz(text, return_tensors=&quot;pt&quot;) encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()} outputs = trainer.model(**encoding) . Lets convert the model outputs to probabilities! . logits = outputs.logits logits.shape . torch.Size([1, 54]) . sigmoid = torch.nn.Sigmoid() probs = sigmoid(logits.squeeze().cpu()) . In each column the probability for each grade in category! . probs_cat = torch.reshape(probs,(6,-1)) probs_cat . tensor([[1.3449e-03, 4.0534e-03, 1.9329e-01, 4.4354e-01, 2.9212e-01, 5.5185e-02, 4.2550e-03, 9.5515e-04, 2.5638e-04], [8.3035e-04, 4.3410e-03, 2.6706e-01, 5.0903e-01, 2.1249e-01, 1.7460e-02, 2.2924e-03, 3.5820e-04, 1.7877e-04], [3.1327e-04, 1.2887e-03, 4.9035e-02, 3.4472e-01, 5.9424e-01, 3.5466e-02, 5.9954e-03, 6.2657e-04, 3.3844e-04], [1.1110e-03, 1.2210e-03, 2.2265e-01, 4.9241e-01, 2.9399e-01, 2.3312e-02, 4.7664e-03, 6.3960e-04, 3.5847e-04], [7.9150e-04, 3.1421e-03, 3.5284e-01, 4.5543e-01, 1.6734e-01, 2.3337e-02, 3.6956e-03, 4.7873e-04, 3.8009e-04], [1.8744e-03, 2.8594e-03, 2.6097e-01, 4.9264e-01, 1.8876e-01, 3.0931e-02, 2.9663e-03, 5.4795e-04, 3.0063e-04]], grad_fn=&lt;ReshapeAliasBackward0&gt;) . Find for each category the predicted grade! . pred = torch.argmax(probs_cat, axis=1) pred . tensor([3, 3, 4, 3, 3, 3]) . predicted_labels = [float(1 + 0.5*i) for i in pred] predicted_labels . [2.5, 2.5, 3.0, 2.5, 2.5, 2.5] . preds = trainer.predict(eval_ds) . ***** Running Prediction ***** Num examples = 3 Batch size = 8 . . [1/1 : &lt; :] preds_tt = torch.Tensor(preds.predictions) . sigmoid = torch.nn.Sigmoid() probs = sigmoid(preds_tt) . probs.shape . torch.Size([3, 54]) . probs_cat = torch.reshape(probs,(eval_ds.num_rows,6,-1)) probs_cat . tensor([[[1.3449e-03, 4.0534e-03, 1.9329e-01, 4.4354e-01, 2.9212e-01, 5.5185e-02, 4.2550e-03, 9.5515e-04, 2.5638e-04], [8.3035e-04, 4.3410e-03, 2.6706e-01, 5.0903e-01, 2.1249e-01, 1.7460e-02, 2.2924e-03, 3.5820e-04, 1.7877e-04], [3.1327e-04, 1.2887e-03, 4.9035e-02, 3.4472e-01, 5.9424e-01, 3.5466e-02, 5.9954e-03, 6.2657e-04, 3.3844e-04], [1.1110e-03, 1.2210e-03, 2.2265e-01, 4.9241e-01, 2.9399e-01, 2.3312e-02, 4.7664e-03, 6.3960e-04, 3.5847e-04], [7.9150e-04, 3.1421e-03, 3.5284e-01, 4.5543e-01, 1.6734e-01, 2.3337e-02, 3.6956e-03, 4.7873e-04, 3.8009e-04], [1.8744e-03, 2.8594e-03, 2.6097e-01, 4.9264e-01, 1.8876e-01, 3.0931e-02, 2.9663e-03, 5.4795e-04, 3.0063e-04]], [[6.9702e-04, 2.2156e-03, 1.3525e-01, 4.0190e-01, 3.5979e-01, 8.3393e-02, 6.3948e-03, 8.6993e-04, 2.2647e-04], [4.6869e-04, 2.0869e-03, 1.7277e-01, 5.0589e-01, 2.9519e-01, 2.7507e-02, 3.1139e-03, 3.2655e-04, 1.5393e-04], [1.7464e-04, 7.4361e-04, 2.6913e-02, 2.8327e-01, 6.5481e-01, 5.2866e-02, 8.4536e-03, 6.0302e-04, 2.9023e-04], [5.6700e-04, 7.7239e-04, 1.4403e-01, 4.4338e-01, 3.8536e-01, 3.7247e-02, 6.1888e-03, 6.0653e-04, 3.1055e-04], [4.2745e-04, 1.7662e-03, 2.5455e-01, 4.5494e-01, 2.3762e-01, 3.6414e-02, 5.2301e-03, 4.7578e-04, 3.3699e-04], [9.9297e-04, 1.4795e-03, 1.5690e-01, 4.7478e-01, 2.8432e-01, 5.2038e-02, 4.1617e-03, 5.1195e-04, 2.6572e-04]], [[8.9919e-05, 2.2792e-04, 1.0313e-02, 9.4493e-02, 3.4430e-01, 4.0744e-01, 1.2611e-01, 4.3038e-03, 1.0102e-03], [7.9191e-05, 1.3719e-04, 1.1439e-02, 7.6456e-02, 5.0715e-01, 3.4287e-01, 5.0529e-02, 2.9594e-03, 5.8506e-04], [5.5959e-05, 1.3887e-04, 1.4530e-03, 2.7991e-02, 4.2644e-01, 4.2822e-01, 1.2768e-01, 4.4846e-03, 1.3150e-03], [8.3560e-05, 1.8322e-04, 8.9937e-03, 6.9328e-02, 4.2182e-01, 3.8917e-01, 8.6680e-02, 3.8928e-03, 1.0506e-03], [7.3068e-05, 2.1460e-04, 1.5649e-02, 1.3015e-01, 3.8366e-01, 3.6370e-01, 8.9219e-02, 3.5911e-03, 1.2481e-03], [9.8681e-05, 1.5562e-04, 4.8098e-03, 6.4202e-02, 4.5954e-01, 3.9083e-01, 8.0475e-02, 5.0407e-03, 1.0509e-03]]]) . pred = torch.argmax(probs_cat, axis=2) pred . tensor([[3, 3, 4, 3, 3, 3], [3, 3, 4, 3, 3, 3], [5, 4, 5, 4, 4, 4]]) . If we get prediction for the i&#39;th column that means we have have for the category given by the row index . $grad_{cat} = i * 0.5 + 1$ . grades = pred*0.5+1 grades . tensor([[2.5000, 2.5000, 3.0000, 2.5000, 2.5000, 2.5000], [2.5000, 2.5000, 3.0000, 2.5000, 2.5000, 2.5000], [3.5000, 3.0000, 3.5000, 3.0000, 3.0000, 3.0000]]) . Let&#39;s convert this to a dataframe: . sub = pd.DataFrame(grades.numpy()) sub.columns = df_ss.columns[1:] sub[&quot;text_id&quot;] = ds_tst[&quot;text_id&quot;] sub . cohesion syntax vocabulary phraseology grammar conventions text_id . 0 2.5 | 2.5 | 3.0 | 2.5 | 2.5 | 2.5 | 0000C359D63E | . 1 2.5 | 2.5 | 3.0 | 2.5 | 2.5 | 2.5 | 000BAD50D026 | . 2 3.5 | 3.0 | 3.5 | 3.0 | 3.0 | 3.0 | 00367BB2546B | . df_ss . text_id cohesion syntax vocabulary phraseology grammar conventions . 0 0000C359D63E | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . 1 000BAD50D026 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . 2 00367BB2546B | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | 3.0 | . We need to rearrange the columns to get the right format. . sub = sub[df_ss.columns] sub . text_id cohesion syntax vocabulary phraseology grammar conventions . 0 0000C359D63E | 2.5 | 2.5 | 3.0 | 2.5 | 2.5 | 2.5 | . 1 000BAD50D026 | 2.5 | 2.5 | 3.0 | 2.5 | 2.5 | 2.5 | . 2 00367BB2546B | 3.5 | 3.0 | 3.5 | 3.0 | 3.0 | 3.0 | . &lt;/div&gt; .",
            "url": "https://simveit.github.io/ai_blog/2022/09/05/First-try-with-Hugging-Face.html",
            "relUrl": "/2022/09/05/First-try-with-Hugging-Face.html",
            "date": " ‚Ä¢ Sep 5, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://simveit.github.io/ai_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://simveit.github.io/ai_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://simveit.github.io/ai_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}